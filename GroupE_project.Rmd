---
title: "GroupE Project"
author: "A. Gottardi, A. Minutolo, E. Corrolezzis, L.F. Palacios Flores"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
editor_options:
  markdown:
    wrap: 72
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
if(!require("pacman")) install.packages("pacman")
pacman::p_load(patchwork, GGally, ggplot2, car, mgcv, skimr, viridis, pROC, caret, purrr, mgcViz, psych, gridExtra, dplyr, MASS, randomForest, yardstick)
```

```{r, include=FALSE}
current_path <- getwd()
datasets_dir <- file.path(current_path, "datasets")
load(paste(datasets_dir, "train_reduced.RData", sep = "/"))
load(paste(datasets_dir, "unbalanced_train.RData", sep = "/"))
load(paste(datasets_dir, "unbalanced_test.RData", sep = "/"))
```

"Doubt the data until the data leave no room for doubt." - Henri
Poincaré

# Problem statement

The dataset contains the data of the clients of an Insurance company
that has provided Health Insurance. Our goal is to analyze the
relationship between the features and the probability of the customers
buying a vehicle insurance. Now, in order to predict whether the
customer would be interested in Vehicle insurance, we have information
about demographics (gender, age, region code type), Vehicles (Vehicle
Age, Damage), Policy ins(Premium, sourcing channel) etc.

Our client is an Insurance company that has provided Health Insurance to
its customers. Now they need the help in building a model to predict
whether the policyholders (customers) from the past year will also be
interested in Vehicle Insurance provided by the company.

An insurance policy is an arrangement by which a company undertakes to
provide a guarantee of compensation for specified loss, damage, illness,
or death in return for the payment of a specified premium. A premium is
a sum of money that the customer needs to pay regularly to an insurance
company for this guarantee.

Building a model to predict whether a customer would be interested in
Vehicle Insurance is extremely helpful for the company because it can
then accordingly plan its communication strategy to reach out to those
customers and optimize its business model and revenue.

# Data

We had three datasets to analyze 'train.csv', 'test.csv' and
'sample.csv'. Among these datasets we only analyzed the first one. The
'test.csv' dataset lacked the 'Response' variable and the 'sample.csv'
file contained observations of this variable but one for one category,
making them unusable.

Our dataset is composed of the following variables:

| Variable             | Definition                                                                                                                  | Type        |
|-------------------|----------------------------------|-------------------|
| id                   | Unique ID for the customer                                                                                                  | Numeric     |
| Gender               | Gender of the customer                                                                                                      | Categorical |
| Age                  | Age of the customer                                                                                                         | Numeric     |
| Driving_License      | 0 : Customer does not have DL, 1 : Customer already has DL                                                                  | Binary      |
| Region_Code          | Unique code for the region of the customer                                                                                  | Categorical |
| Previously_Insured   | 1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance                                     | Binary      |
| Vehicle_Age          | Age of the Vehicle                                                                                                          | Categorical |
| Vehicle_Damage       | 1 : Customer got his/her vehicle damaged in the past. 0 : Customer didn't get his/her vehicle damaged in the past.          | Binary      |
| Annual_Premium       | The amount customer needs to pay as premium in the year                                                                     | Numeric     |
| Policy_Sales_Channel | Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc. | Categorical |
| Vintage              | Number of Days, Customer has been associated with the company                                                               | Numeric     |
| Response             | 1 : Customer is interested, 0 : Customer is not interested                                                                  | Binary      |

The first step is trying to get some insights about the dataset by
plotting and analyzing the data. We present the barplots for the
categorical variables and the density plots for the numerical ones.

The 'id' variable is just a discrete ordered variable with uniform
distribution. Therefore, we just removed it from our analysis.

## Exploratory Data Analysis

### 'Response' variable

The proportion for the categories of the response variable are the
following:

```{r, echo = FALSE}
# Pie chart of Response variable with percentages of the total observations. 
# Calculate the percentages
response_counts <- table(train_reduced$Response)
response_props <- round(prop.table(response_counts) * 100, 2)

# Create a data frame for the pie chart
pie_data <- data.frame(
  Response = factor(names(response_counts), labels = c("No", "Yes")),
  Count = as.numeric(response_counts),
  Label = paste0(response_props, "%")
)

# Create the pie chart
p0 <- ggplot(pie_data, aes(x = "", y = Count, fill = Response)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  labs(x = "", y = "", fill = "Interest in Car Insurance") +
  theme_void() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 12), 
    text = element_text(size = 12)
  ) +
  geom_text(
    aes(y = Count / 2, label = Label),
    position = position_stack(vjust = 0.9),
    size = 8
  ) +
  scale_fill_discrete(labels = c("No", "Yes"))
p0
```

As we can see the dataset is unbalanced and the imbalance ratio is:

```{r, echo = FALSE}
props <- table(train_reduced$Response)
IR<-as.numeric(props[1]/props[2])
sprintf("IR: %f", IR)
```

This degree of imbalance is considered to be weak with respect to the
reference level of 10 for slight imbalance, thus we decided to not
perform any procedure to correct the imbalance.

### Numerical variables

```{r, fig.width=15, fig.height=15, echo=FALSE}
p2 <- ggpairs(train_reduced, 
              columns = c("Age", "Annual_Premium", "Vintage"),
              aes(color = Response),
              diag = list(discrete="barDiag", 
                          continuous = wrap("densityDiag", alpha=0.7)))
p2
```

The distribution of the Age variable with respect to the Response
variable shows that the majority of the customers who are interested in
car insurance are middle-aged (between 30 and 60 years old), which
coincides with the age people are more likely to own a car. The
customers not interested in acquiring a car insurance policy are mostly
distributed among younger people and some middle-aged adults in their
50s. The ditribution is skewed to the right.

The plot for Annual Premium suggests that the costs of the car insurance
policy is independent of the interest if the customers to buy the
product. It has a highly right-skewed distribution, with most of the
data concentrated on the lower end of the premium scale and a long tail
extending to higher premium values. The lower tail shows a high values
as a consequence of entry level health insurance policy as expected.

Since both Age and Annual Premium are skewed to the right, we considered
to apply a logarithm transformation for both the variables.

```{r, fig.width=15, fig.height=15, echo = FALSE}
train_reduced$logAge <- log(train_reduced$Age)
train_reduced$logAnnual_Premium <- log(train_reduced$Annual_Premium)
p3 <- ggpairs(train_reduced, 
              columns = c("logAge", "logAnnual_Premium"),
              aes(color = Response),
              diag = list(discrete="barDiag", 
                continuous = wrap("densityDiag", alpha=0.7 )))
p3
```

The plot for Vintage shows a nearly uniform distribution, with a slight
increase in frequency towards the middle range of the Vintage variable.
It may not be significant in the explanation of the Response.

The variables show negligible linear correlation between them, which is
clearly shown in the scatter plot and in the correlation coefficients.

### Categorical variables

```{r, fig.width=15, fig.height=9, echo = FALSE}
p9 <- ggplot(train_reduced, aes(x = Region_Code, fill = Response)) + 
  geom_bar(position = "dodge", color = "black") +
  labs(y = "Count", fill = "Response") +
  scale_x_discrete(breaks = function(x) x[seq(1, length(x), by = 10)]) +
  theme(
    text = element_text(size = 12),
    axis.text = element_text(size = 14)
  )


p14 <- ggplot(train_reduced, aes(x = Policy_Sales_Channel, fill = Response)) + 
  geom_bar(position = "dodge", color = "black") +
  labs(y = "Count", fill = "Response") +
  scale_x_discrete(breaks = function(x) x[seq(1, length(x), by = 20)]) +
  theme(
    text = element_text(size = 12),
    axis.text = element_text(size = 14)
  )
p_cat <- p9 + p14
p_cat##Should be clearer
```

Regarding the variable Region_Code, we can notice that the vast majority
of the customers are from region 28. The customers from region 28 are
also the ones who are most interested in car insurance. Almost half of
the customers are distributed among regions 8, 28, 41, 46 accounting for
\~47% of the total customers. Since this variable has a lot of labels
with low frequency, we decided to consider only the major four ones
mentioned above and an additional one with the remaining labels as a
unique category.

Also in Policy_Sales_Channel, there are four categories more frequent
than others: Channels 26, 124, 152 and 160 alone account for more than
80% of the customers. Channels 26 and 124 are the ones with the highest
percentage of customers interested in the product. Only about 20% of the
customers interested in the product are distributed in the rest of the
channels of outreach. As we did for Region_Code, we grouped the
remaining less frequent categories as one.

After the grouping:

```{r, echo = FALSE}
p10 <- ggplot(train_reduced, aes(x = Region_Reduced, fill = Response)) + 
  geom_bar(position = "dodge", color = "black") +
  labs(y = "Count", fill = "Response") +
  theme(
    text = element_text(size = 12),
    axis.text = element_text(size = 14)
  ) + 
  guides(fill = FALSE)
p15 <- ggplot(train_reduced, aes(x = Channels_Reduced, fill = Response)) + 
  geom_bar(position = "dodge", color = "black") +
  labs(y = "Count", fill = "Response") +
  theme(
    text = element_text(size = 12),
    axis.text = element_text(size = 14)
  ) + 
  guides(fill = FALSE)
p10+p15
```

The data for Gender shows a similar trend for the interest of males and
females customers in the product. However, there is a statistically
significant difference in their proportions.

```{r}
prop.test(table(train_reduced$Gender, train_reduced$Response))
```

```{r, echo = FALSE}
p7 <- ggplot(train_reduced, aes(x = Gender, fill = Response)) + 
  geom_bar(position = "fill", color = "black") +
  labs(y = "Percentage", fill = "Response") +
  scale_y_continuous(labels = scales::percent_format()) +
  theme(
    text = element_text(size = 12),
    axis.text = element_text(size = 14)
  ) +
  guides(fill = FALSE)

p7
```

# MODELS

After exploring the data, now we proceed with the fitting and assessment
of different models for binary classification. For training and testing
the models we performed a static train/test split with 70% of train set
and 30% of test set.

In order to understand which variables are more significant in the
explanation of the response variable, we analyzed nested models with
different combinations of selected explanatory variables.

## GLM

### stepAIC

The first approach we used consists of using the function stepAIC() from
the MASS package to find the best combination of predictors with respect
to AIC. The stepAIC() function must be applied to the full model, which
serves as the starting point for the variable selection process. We
chose the 'both' direction, that considers both adding and removing
variables from the model.

```{r, echo = FALSE}
# Drop columns Policy_Sales_Channel and Region_Code from unbalanced_train
unbalanced_train <- unbalanced_train[, !names(unbalanced_train) %in% c("Policy_Sales_Channel", "Region_Code")]

# Drop columns Policy_Sales_Channel and Region_Code from unbalanced_test
unbalanced_test <- unbalanced_test[, !names(unbalanced_test) %in% c("Policy_Sales_Channel", "Region_Code")]
```

```{r}
full_model <- glm(Response ~ Gender + Age + Driving_License + Previously_Insured + 
    Vehicle_Age + Vehicle_Damage + Annual_Premium + Vintage + 
    Channels_Reduced + Region_Reduced, data = unbalanced_train, family = binomial)
stepAIC(full_model, direction = 'both')
```

Performing the stepAIC() function we confirmed that the variable Vintage
is not useful for the model, because of its uniform distribution. The
stepAIC() function stops when the ranking of models built by removing
and adding one variable at a time has an AIC greater than the default
model. Hence, this procedure doesn't allow us to obtain a simple model
based on the Occam's razor. Consequentially we also implemented a
procedure to obtain a reduced model based on idea of the stepAIC().

```{r}
best_model<-glm(Response ~ Gender + Age + Driving_License + Previously_Insured  + Vehicle_Age + Vehicle_Damage + Annual_Premium + 
    Channels_Reduced + Region_Reduced, data = unbalanced_train, family = binomial)
summary(best_model)
```

```{r}
exp(best_model$coefficients) 
```

These results suggest that:

-   Male customers are 9% more interested in car insurance than female
    customers.

-   There is a slight decrease in the odds of interest in the product
    with each year of increasing customer age.

-   Customers with a driver's license are 3 times more interested in the
    product than those without a driver's license.

-   There is a dramatic decrease in the interest of the customers in the
    product for those who previously had their cars insured. The company
    should focus on improving its services because current policyholders
    lose their interest by 98% with respect to those without car
    insurance.

-   The results show that customers with older cars show more interest
    than customers with new cars. Around 2 times for cars more than 2
    years old compared to new cars.

-   The interest in the Car Insurance Policy of the customers with
    health insurance who had their car damaged in the past is more than
    7 times that of those who haven´t. This is expected and a variable
    the company should focus on to estimate the risk associated with
    these customers.

-   The amount the customer needs to pay as a premium in the year
    doesn't seem to be associated with any increase or decrease in the
    odds of the event.

-   There is more interest in the product, from \~14% to \~54% more
    interest, for customers from Regions 28, 41, 46, and 0 (combination
    of low frequent regions) with respect to Region 8.

-   The outreach Channel 26 (base category in the model) gets from 16%
    to 90% more interested customers in comparison with Channel 26.
    Channel 124 attracts a lot of clients as well and the company could
    attempt to increase its influence in this channel.

-   The variable Vintage (number of days the customers have been
    associated with the company) was completely removed from the model
    because of low significance with respect to the AIC.

### Nested models

This procedure builds a ranking of variables, similar to the one of the
stepAIC(), meaning to create models by removing one variable at a time
and sorting the variables by AIC. After that, we built nested models by
adding one variable at a time based on the ranking of variables
mentioned before. The aim is to find the optimal model based on both AIC
and the Occam's razor.

```{r}
#*--------------------------------------------------------------------
#*FUNCTIONS 
#*--------------------------------------------------------------------

# * This function assumes that `Policy_Sales_Channel` and `Region_Code` have been removed from the data
ranking_nested_models <- function(train_data, test_data, use_model = "glm", use_log = TRUE, use_splines = FALSE) {
  if(use_model == "glm" && use_log == TRUE){
    numeric_variables <- c("I(log(Age))", "I(log(Annual_Premium))")
  } else if (use_model == "glm" && use_log == FALSE){
    numeric_variables <- c("Age", "Annual_Premium")
  } else if (use_model == "gam" && use_log == TRUE && use_splines == TRUE){
    numeric_variables <- c("s(I(log(Age)))", "s(I(log(Annual_Premium)))")
  } else if (use_model == "gam" && use_log == TRUE && use_splines == FALSE){
    numeric_variables <- c("I(log(Age))", "I(log(Annual_Premium))")
  } else if (use_model == "gam" && use_log == FALSE && use_splines == TRUE){
    numeric_variables <- c("s(Age)", "s(Annual_Premium)")
  } else if (use_model == "gam" && use_log == FALSE && use_splines == FALSE){
    numeric_variables <- c("Age", "Annual_Premium")
  }

  #*VARIABLES IMPORTANCE RANKING --------------------------------------------------

  # Sort variables by importance wrt AIC
  # We remove one variable at a time and by decreasing AIC we get the most important variables
  # i.e., the variables that when removed increase the AIC is important
  predictors <- colnames(train_data)
  predictors <- predictors[predictors != 'Response']
  ranking_variables_models <- list()
  sum_variables <- paste(predictors, collapse = " + ")

  for (predictor in predictors){
    if (predictor == "Age"){
      formula_string <- paste("Response ~", sum_variables ,"- Annual_Premium -", predictor, "+", numeric_variables[2])

    } else if (predictor == "Annual_Premium"){
      formula_string <- paste("Response ~", sum_variables ,"- Age -", predictor, "+", numeric_variables[1])
    } else {
      formula_string <- paste("Response ~", sum_variables ,"- Age - Annual_Premium -", predictor, "+", paste(numeric_variables, collapse = " + "))

    }
    model_formula <- as.formula(formula_string)

    if(use_model == "glm"){
      model <- glm(model_formula, data = train_data, family = binomial)
    } else if (use_model == "gam"){
      model <- gam(model_formula, data = train_data, family = binomial)
    }

    ranking_variables_models[[predictor]] <- model
  }

  # Compute AIC values
  ranking_variables_aic_values <- sapply(ranking_variables_models, AIC)

  # Sort variables by AIC values
  df_ranking_variables_aic <- data.frame(VariableRemoved = predictors, AIC = ranking_variables_aic_values)
  # Assuming df is your DataFrame
  df_sorted_ranking_variables_aic <- df_ranking_variables_aic[order(df_ranking_variables_aic$AIC, decreasing=TRUE), ]
  # df_sorted_ranking_variables_aic

  #* NESTED MODELS ----------------------------------------------------------------
  variables_order <- df_sorted_ranking_variables_aic$VariableRemoved
  # variables_order
  variables_nested <- c()
  nested_models <- list()

  for (variable in variables_order) {
    if (variable == "Age"){
      variables_nested <- c(variables_nested, numeric_variables[1])
    } else if (variable == "Annual_Premium"){
      variables_nested <- c(variables_nested, numeric_variables[2])
    } else {
      variables_nested <- c(variables_nested, variable)
    }
    formula_string <- paste("Response", "~", paste(variables_nested, collapse = " + "))
    print(formula_string)  
    model_formula <- as.formula(formula_string)
    if(use_model == "glm"){
      model <- glm(model_formula, data = train_data, family = binomial)
    } else if (use_model == "gam"){
      model <- gam(model_formula, data = train_data, family = binomial)
    }
    
    nested_models[[variable]] <- model
  }

  # Compute AIC values
  raking_nested_models_aic_values <- sapply(nested_models, AIC)
  df_ranking_nested_models_aic <- data.frame(Model_Name = variables_order, AIC = raking_nested_models_aic_values)
  # df_ranking_nested_models_aic

  # Sort variables by AIC values
  df_sorted_ranking_nested_models_aic <- df_ranking_nested_models_aic[order(df_ranking_nested_models_aic$AIC, decreasing=TRUE), ]

  # COMPUTE AUC AND ACCURACY FOR EACH MODEL -------------------------------------

  # Apply models_assessment function to each model using map
  results_list <- map(nested_models, ~models_assessment(.x, test_data))

  # Compute AUC values
  auc_values <- list()
  accuracy_values <- list()
  tpr_values <- list()
  fpr_values <- list()
  tnr_values <- list()
  fnr_values <- list()
  precision_values <- list()
  threshold_values <- list()

  for (i in 1:length(results_list)){
    auc_values <- c(auc_values, as.numeric(results_list[[i]][1]))
    accuracy_values <- c(accuracy_values, as.numeric(results_list[[i]][2]))
    tpr_values <- c(tpr_values, as.numeric(results_list[[i]][3]))
    fpr_values <- c(fpr_values, as.numeric(results_list[[i]][4]))
    tnr_values <- c(tnr_values, as.numeric(results_list[[i]][5]))
    fnr_values <- c(fnr_values, as.numeric(results_list[[i]][6]))
    precision_values <- c(precision_values, as.numeric(results_list[[i]][7]))
    threshold_values <- c(threshold_values, as.numeric(results_list[[i]][8]))
  }

  result_df <- data.frame(
    Model_Name = df_ranking_nested_models_aic$Model_Name, 
    AIC = df_ranking_nested_models_aic$AIC,
    AUC = unlist(auc_values),
    Accuracy = unlist(accuracy_values),
    TPR = unlist(tpr_values),
    FPR = unlist(fpr_values),
    TNR = unlist(tnr_values),
    FNR = unlist(fnr_values),
    Precision = unlist(precision_values),
    Threshold = unlist(threshold_values)
  )

  # Return ranking of variables, | dataframe
  # results | dataframe
  # and nested models | list
  return(list(Ranking_Variables = df_sorted_ranking_variables_aic, Results = result_df, Models = nested_models))

}


#*FUNCTION TO PERFORM THE MODEL ASSESSMENT -------------------------------------
models_assessment <- function(model, test_data){
  # Predict probabilities
  probabilities <- predict(model, newdata = subset(test_data, select = -Response), type = "response")

  # Compute ROC curve
  roc_curve <- roc(test_data$Response, probabilities)
  
  # Calculate AUC
  auc_score <- auc(roc_curve)

  # Find optimal threshold using Youden's J statistic
  youdens_j <- coords(roc_curve, "best", best.method = "youden")
  optimal_threshold <- youdens_j$threshold


  # Obtain predicted classes based on the optimal threshold
  predicted_classes <- ifelse(probabilities > optimal_threshold, "Yes", "No")

  # Create the confusion matrix
  conf_matrix <- table(Actual = test_data$Response, Predicted = predicted_classes)

  conf_matrix_prop <- prop.table(conf_matrix, margin = 1)

  # Calculate accuracy
  accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

  # Calculate true positive rate
  tpr <- conf_matrix[2, 2] / sum(conf_matrix[2, ])

  # Calculate false positive rate
  fpr <- conf_matrix[1, 2] / sum(conf_matrix[1, ])

  # Calculate true negative rate
  tnr <- conf_matrix[1, 1] / sum(conf_matrix[1, ])

  # Calculate false negative rate
  fnr <- conf_matrix[2, 1] / sum(conf_matrix[2, ])

  # Calculate precision
  precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])

  return(list(auc_score = auc_score, accuracy = accuracy,
              tpr = tpr, fpr = fpr, tnr = tnr, fnr = fnr,
              precision = precision, optimal_threshold = optimal_threshold))

}

```

```{r}
result_glm <- ranking_nested_models(unbalanced_train, unbalanced_test, use_model = "glm", use_log = FALSE, use_splines = FALSE)
result_glm$Results
```

The ranking of variables shown has similar results to the one of the
stepAIC(). We also performed an Anova test in order to understand better
the improvements of the nested models.

Anova test:

```{r}

# Perform ANOVA on full model

anova_values <- anova(result_glm$Models$Vintage, test = "Chisq")
anova_values
```

Observing the AIC and the anova test we noticed that there is not a
significant decrease in the AIC or a significant improvement in deviance
in the last three models, hence we decided to consider the nested model
up to Region_Reduced.

```{r}
summary(result_glm$Models$Region_Reduced)
```

As we can see from the summary, all the variables have a p-value close
to 0, hence they are all statistically significant. We can compute the
exponential of these coefficients to obtain the odds ratio:

```{r}
exp(result_glm$Models$Region_Reduced$coefficients)
```

we can comment these results as follows:

-   The odds of a customer being interested in Vehicle insurance are 7
    times higher for those who have had a vehicle damage compared to
    those who didn't.

-   Compared to region 8, the odds of a customer being interested in
    Vehicle insurance are higher for those who live in Region 28, 41, 46
    or 0.

-   The odds of a customer being interested in Vehicle insurance get
    higher as the age of the vehicle increases.

-   Compared to channel 26, the odds of a customer being interested in
    vehicle insurance are lower for those having a channel of
    outreaching with code 124, 160, 152 or 0.

-   The odds of a customer being interested in vehicle insurance get
    lower for:

    -   older customers;
    -   customers that have been previously insured.

We also analyzed the model including a logarithm transformation for the
Age:

```{r}
result_glm_log <- ranking_nested_models(unbalanced_train, unbalanced_test, use_model = "glm", use_log = TRUE, use_splines = FALSE)
result_glm_log$Results
```

This analysis shows that the logarithm transformation doesn't produce a
better AIC, actually it performes worse than the model without
logarithm. Hence, we decided not to include the logarithm
transformation.

```{r}
vif(result_glm$Models$Driving_License)
```

Through the vif function we can see that there is no sign of
multicolinearity, since the variance inflation factors are very small
for every variable.

```{r}

# Get predicted values from the model
predicted_values <- predict(result_glm$Models$Region_Reduced, unbalanced_test, type = "response")
# Calculate residuals
residuals <- residuals(result_glm$Models$Region_Reduced, type = "response")
 
# Plot the binned residuals
arm::binnedplot(predicted_values, residuals)
```

The binned residuals shown are contained in the confidence interval and
they are evenly concentrated around zero. There seem to be a slight hint
of heteroscedasticity.

## GAM

In order to fit GAM models we performed the ranking of nested models
used for GLMs:

```{r}
result_gam <- ranking_nested_models(unbalanced_train, unbalanced_test, use_model = "gam", use_log = FALSE, use_splines = TRUE)
result_gam$Results
```

Fitting a GAM model with a spline for the variable Age improves the AIC
of the models. Therefore, the comparison of the AIC of the nested GAM
models confirms the results obtained with GLM: that is, the model that
strikes the best balance between AIC value and number of variables is
the one that contains the variables up to Region_Reduced; adding other
variables doesn't significantly reduce the AIC. It's worth to notice
that the order of the variables changes: Age results to be slightly more
significant adding the spline.

```{r}
summary(result_gam$Models$Region_Reduced)
```

Since the expected degrees of freedom of the variable Age is 8.34, we
can consider the splines relevant for the variable.

From the summary we can see that all the variables are statistically
significant, hence we may interpret their meaning by analyzing the
exponential values:

```{r}
exp(result_gam$Models$Region_Reduced$coefficients)
```

We can comment these results as follows:

-   The odds of a customer being interested in Vehicle insurance are 7
    times higher for those who have had a vehicle damage compared to
    those who didn't.

-   Compared to region 8, the odds of a customer being interested in
    Vehicle insurance are higher for those who live in Region 28, 41, 46
    or 0.

-   The odds of a customer being interested in Vehicle insurance get
    higher as the age of the vehicle increases.

-   Compared to channel 26, the odds of a customer being interested in
    vehicle insurance are lower for those having a channel of
    outreaching with code 124, 160, 152 or 0.

-   The odds of a customer being interested in vehicle insurance get
    lower for:

    -   older customers;
    -   customers that have been previously insured.

Trying to fit the model with gam and adding a spline for Age variable,
we noticed that not only the AIC improves, but the expected degrees of
freedom for the Age are significantly high, hence, so far, the best
model seems to be the one using gam with splines on Age.

```{r}

# Get predicted values from the model
predicted_values <- predict(result_gam$Models$Region_Reduced, unbalanced_test, type = "response")

# Calculate residuals
residuals <- residuals(result_glm$Models$Region_Reduced, type = "response")

# Plot the binned residuals
arm::binnedplot(predicted_values, residuals)
```

The binned residuals shown are contained in the confidence interval and
they are evenly concentrated around zero. There seem to be a slight hint
of heteroscedasticity.

```{r}
# select the commands that actually convey relevant information
gam_sampledViz <- getViz(results_gam$Models$Region_Reduced)
print(plot(gam_sampledViz, allTerms = T), pages = 1)
```

This plot shows the log-odds coefficients of the GAM model for the
categorical variables and the spline of Age.

## Random Forest

Due to our limited computational resources, we had to sample our dataset
in order to fit a random forest model with 500 trees.

```{r}
sample <- train_reduced[sample(nrow(train_reduced), nrow(train_reduced)*0.6, replace = FALSE),]
```

```{r}
trainIndex <- createDataPartition(sample$Response, p = .8, list = FALSE, times = 1)

trainSet <- sample[trainIndex,]
testSet <- sample[-trainIndex,]
```

```{r}
# Drop columns Policy_Sales_Channel and Region_Code from unbalanced_train
trainSet <- trainSet[, !names(trainSet) %in% c("Policy_Sales_Channel", "Region_Code")]

# Drop columns Policy_Sales_Channel and Region_Code from unbalanced_test
testSet <- testSet[, !names(testSet) %in% c("Policy_Sales_Channel", "Region_Code")]
```

```{r}
model_rf <- randomForest(Response ~ ., data = trainSet, importance = TRUE, ntree = 500)
print(model_rf)
```

We can notice that the class error for the Yes category is very high,
therefore we decided to use a threshold in order to avoid
misclassification.

```{r}
# Predict probabilities
probabilities <- predict(model_rf, newdata = subset(testSet, select = -Response), type = "prob")[, "Yes"]
 
# Compute ROC curve
roc_curve <- roc(testSet$Response, probabilities)
 
# Calculate AUC
auc_score <- auc(roc_curve)
 
# Find optimal threshold using Youden's J statistic
youdens_j <- coords(roc_curve, "best", best.method = "youden")
optimal_threshold <- youdens_j$threshold
 
# Plot the ROC curve using plot.roc from the pROC package
plot.roc(roc_curve, col = "blue", main = "ROC Curve", lwd = 2)
 
# Add a point for the best threshold
points(youdens_j$specificity, youdens_j$sensitivity, pch = 19, col = "red")
 
# Adding a legend or text to mark the point
text(youdens_j$specificity, youdens_j$sensitivity, labels = paste("Threshold:", round(optimal_threshold, 2)), pos = 4)
 
# Add labels and legend
abline(h = 0, v = 1, lty = 2, col = "gray")
legend("topright", legend = paste("AUC =", round(auc(roc_curve), 2)), col = "blue", lwd = 2)
 
```

The model has an AUC of over 80% and the threshold which maximizes the
difference between TPR and FPR is around 0.01; this small value could be
justified by the imbalance of the response variable.

```{r}
#*FUNCTION TO PERFORM THE MODEL ASSESSMENT -------------------------------------
random_forest_assessment <- function(model_rf, testSet){
  # Probabilities prediction of the positive class
  probabilities <- predict(model_rf, newdata = subset(testSet, select = -Response), type = "prob")[, "Yes"]

  # Compute ROC curve
  roc_curve <- roc(testSet$Response, probabilities)
  
  # Calculate AUC
  auc_score <- auc(roc_curve)

  # Find optimal threshold using Youden's J statistic
  youdens_j <- coords(roc_curve, "best", best.method = "youden")
  optimal_threshold <- youdens_j$threshold

  # Save ROC curve plot if specified

  # Obtain predicted classes based on the optimal threshold
  predicted_classes <- ifelse(probabilities > optimal_threshold, "Yes", "No")

  # Create the confusion matrix
  conf_matrix <- table(Actual = testSet$Response, Predicted = predicted_classes)

  conf_matrix_prop <- prop.table(conf_matrix, margin = 1)

  # Calculate accuracy
  accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

  # Calculate true positive rate
  tpr <- conf_matrix[2, 2] / sum(conf_matrix[2, ])

  # Calculate false positive rate
  fpr <- conf_matrix[1, 2] / sum(conf_matrix[1, ])

  # Calculate true negative rate
  tnr <- conf_matrix[1, 1] / sum(conf_matrix[1, ])

  # Calculate false negative rate
  fnr <- conf_matrix[2, 1] / sum(conf_matrix[2, ])

  # Calculate precision
  precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])

  # Store the results in a data frame
  results_df <- data.frame(AUC = auc_score,
                          Accuracy = accuracy,
                          TPR = tpr, 
                          FPR = fpr,
                          TNR = tnr,
                          FNR = fnr,
                          Precision = precision,
                          Threshold = optimal_threshold)
  # Return the results
  return(results_df)

}

```

```{r}
# Train the random forest model
rf_model <- randomForest(Response ~ ., data = unbalanced_train, ntree = 100)

# Assess the model using the test set and don't save the plots
rf_assessment <- random_forest_assessment(model_rf, testSet)

rf_assessment
```


```{r}
varImpPlot(model_rf, sort = T, main = "Variable Importance")
```

From this plot we can notice that the variable Vintage is taken into
account as one of the most relevant with respect to Mean Decrease Gini.
Apparently Vintage helps the model in finding pure nodes.

## Performances and conclusion

Our goal was to determine the relationship between the response variable
and the predictor. We selected the best models for each type of
technique and computed the performance indexes which are illustrated in
the table.

| Models/Indexes | AUC      | Accuracy | TPR      | FPR      | TNR      | FNR      | Precision |
|---------|---------|---------|---------|---------|---------|---------|---------|
| GLM            | 0.84     | **0.71** | 0.91     | **0.32** | **0.68** | 0.09     | **0.28**  |
| GAM            | **0.85** | 0.70     | **0.93** | 0.34     | 0.66     | **0.07** | **0.28**  |
| Random Forest  | 0.82     | 0.67     | 0.92     | 0.37     | 0.63     | 0.08     | 0.26      |

**GLM:** best model for Accuracy, FPR, TNR and Precision;

**GAM:** best for AUC, TPR, FNR and Precision;

**Random Forest:** doesn't perform better than the other kind of models
under any index, but has similar performances.

After analyzing the upper table and taking into account all the previous
considerations, we can conclude that the model that best explains this
relationship is the GAM model. The relationship is well explained using
the variables Previously_Insurance, Vehicle_Damage, Age,
Channels_Reduced and Region_Code; other variable could be taken into
account but they would over complicate the model while not affecting
significantly the performances. Also we saw that it is significant to
consider splines for the age variable.

The other models obtain good results in respect to the GAM, with GLM
being the closest one and also the lightest to train.

It is worth noting that in the context of insurance, it is sensible to assume that the cost of the FNs is is higher than the cost of FPs, because the unrealized revenues of losing a potential client are greater than the cost of contacting a non interested customer. Therefore, given that the GAM model exhibits the highest AUC and the lowest FNR, this could provide an additional reason for selecting it over the other models.

Finally, it's worth mentioning that we also tried to balance the dataset, but the results were not
significantly different from the unbalanced one. This is what we expected, considering that the
imbalance ratio (IR) of the response variable is not too high.
